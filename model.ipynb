{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0daaa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/eeg_new/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from mamba_ssm import Mamba\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d864bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL_DEFAULT = 256\n",
    "N_LAYERS_DEFAULT = 8\n",
    "N_CHANNELS_DEFAULT = 129\n",
    "PATCH_SIZE_DEFAULT = 10\n",
    "\n",
    "D_STATE_DEFAULT = 16\n",
    "EXPAND_DEFAULT = 2\n",
    "D_CONV_DEFAULT = 4\n",
    "\n",
    "\n",
    "VICREG_LAMBDA_DEFAULT = 25.0\n",
    "VICREG_MU_DEFAULT = 25.0\n",
    "VICREG_NU_DEFAULT = 1.0\n",
    "\n",
    "MDN_COMPONENTS_DEFAULT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec32d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Patch Embedding Layer ---\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    EEG Patch Embedding.\n",
    "    Takes (Batch, Channels, Time) -> (Batch, NumPatches, EmbedDim)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels=N_CHANNELS_DEFAULT, \n",
    "                        embed_dim=D_MODEL_DEFAULT, \n",
    "                        patch_size=PATCH_SIZE_DEFAULT):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv1d(\n",
    "            n_channels,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # (B, D_MODEL, NumPatches)\n",
    "        x = x.permute(0, 2, 1)  # (B, NumPatches, D_MODEL)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f27ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "class NeuroBiMambaBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bi-directional Mamba block with a depthwise 1D conv front-end and a gated MLP-style\n",
    "    projection. Input/Output shape: (B, L, d_model).\n",
    "\n",
    "    Workflow:\n",
    "      1. LayerNorm -> linear projection that produces two halves:\n",
    "         - conv_input (for depthwise conv + activation)\n",
    "         - gate_tensor (used to gate Mamba output)\n",
    "      2. Depthwise conv applied along the sequence dimension (causally cropped).\n",
    "      3. Run Mamba forward on the activated conv output and also on the reversed\n",
    "         sequence to get a backward context; concatenate them.\n",
    "      4. Gate the concatenated mamba output, project back to d_model and add residual.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_model: int = D_MODEL_DEFAULT,\n",
    "                 d_state: int = D_STATE_DEFAULT,\n",
    "                 expand: int = EXPAND_DEFAULT,\n",
    "                 d_conv: int = D_CONV_DEFAULT):\n",
    "        super().__init__()\n",
    "        # Hidden dimension after expansion (used for conv & mamba)\n",
    "        self.hidden_dim = d_model * expand\n",
    "\n",
    "        # Project input -> [conv_input | gate_residual]  (shape: 2 * hidden_dim)\n",
    "        self.in_proj = nn.Linear(d_model, 2 * self.hidden_dim, bias=False)\n",
    "\n",
    "        # Depthwise 1D conv: expects (B, hidden_dim, L)\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.hidden_dim,\n",
    "            out_channels=self.hidden_dim,\n",
    "            kernel_size=d_conv,\n",
    "            padding=d_conv - 1,  # causal-style padding, we'll crop to original length\n",
    "            groups=self.hidden_dim,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        # Two Mamba blocks: one for forward context, one for backward (via flip)\n",
    "        self.mamba_fwd = Mamba(d_model=self.hidden_dim, d_state=d_state, \n",
    "                               d_conv=d_conv, expand=1)\n",
    "        self.mamba_bwd = Mamba(d_model=self.hidden_dim, d_state=d_state, \n",
    "                               d_conv=d_conv, expand=1)\n",
    "\n",
    "        # Project concatenated (fwd | bwd) -> d_model\n",
    "        self.out_proj = nn.Linear(2 * self.hidden_dim, d_model, bias=False)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def _split_projection(self, x: torch.Tensor):\n",
    "        # x: (B, L, d_model) -> x_proj: (B, L, 2*hidden_dim)\n",
    "        x_proj = self.in_proj(x)\n",
    "        conv_input, gate = x_proj.chunk(2, dim=-1)  # each (B, L, hidden_dim)\n",
    "        return conv_input, gate\n",
    "\n",
    "    def _conv_activate(self, conv_input: torch.Tensor, seq_len: int):\n",
    "        # conv_input: (B, L, hidden_dim) -> conv expects (B, hidden_dim, L)\n",
    "        y = rearrange(conv_input, \"b l d -> b d l\")\n",
    "        # conv1d with padding may extend length; crop to original sequence length\n",
    "        y = self.conv1d(y)[:, :, :seq_len]\n",
    "        y = rearrange(y, \"b d l -> b l d\")\n",
    "        return self.activation(y)  # (B, L, hidden_dim)\n",
    "\n",
    "    def _run_bi_mamba(self, activated: torch.Tensor):\n",
    "        # activated: (B, L, hidden_dim)\n",
    "        fwd = self.mamba_fwd(activated)  # (B, L, hidden_dim)\n",
    "        # run backward by flipping sequence dimension\n",
    "        bwd_in = torch.flip(activated, dims=[1])\n",
    "        bwd_out = self.mamba_bwd(bwd_in)\n",
    "        bwd = torch.flip(bwd_out, dims=[1])  # restore original order\n",
    "        # concat along feature dim -> (B, L, 2*hidden_dim)\n",
    "        return torch.cat([fwd, bwd], dim=-1)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, L, d_model)\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # 1) linear proj -> conv input + gate tensor\n",
    "        conv_input, gate = self._split_projection(x)\n",
    "\n",
    "        # 2) depthwise conv + activation (preserve seq length)\n",
    "        conv_activated = self._conv_activate(conv_input, seq_len=x.shape[1])\n",
    "\n",
    "        # 3) bi-directional Mamba processing\n",
    "        mamba_out = self._run_bi_mamba(conv_activated)  # (B, L, 2*hidden_dim)\n",
    "\n",
    "        # 4) SPlit mamba_out, apply same gate to both parts\n",
    "        # gated output and projection back to d_model\n",
    "        fwd_out, bwd_out = mamba_out.chunk(2, dim=-1)  # each (B, L, hidden_dim)\n",
    "        gate_activation = self.activation(gate)\n",
    "        gated_fwd = fwd_out * gate_activation\n",
    "        gated_bwd = bwd_out * gate_activation\n",
    "        gated = torch.cat([gated_fwd, gated_bwd], dim=-1)  # (B, L, 2*hidden_dim)\n",
    "\n",
    "        out = self.out_proj(gated)  # (B, L, d_model)\n",
    "\n",
    "        # residual connection\n",
    "        return out + residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922bf080",
   "metadata": {},
   "source": [
    "JEPA Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36b52907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "class EegMambaJEPA(nn.Module):\n",
    "    \"\"\"\n",
    "    JEPA-style backbone using Patch embedding + stacked NeuroBiMambaBlock layers.\n",
    "\n",
    "    Input: (B, C, T)\n",
    "    Output: CLS token embedding -> (B, d_model)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = D_MODEL_DEFAULT,\n",
    "        n_layer: int = N_LAYERS_DEFAULT,\n",
    "        n_channels: int = N_CHANNELS_DEFAULT,\n",
    "        patch_size: int = PATCH_SIZE_DEFAULT,\n",
    "        d_state: int = D_STATE_DEFAULT,\n",
    "        expand: int = EXPAND_DEFAULT,\n",
    "        use_pos_embed: bool = False,\n",
    "        max_len: int = 1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.use_pos_embed = use_pos_embed\n",
    "\n",
    "        # Patch embedding: (B, C, T) -> (B, NumPatches, d_model)\n",
    "        self.patch_embed = PatchEmbed(n_channels, d_model, patch_size)\n",
    "\n",
    "        # Learnable CLS token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "        # Optional positional embeddings (applied after adding CLS)\n",
    "        if self.use_pos_embed:\n",
    "            self.pos_embed = nn.Parameter(torch.randn(1, max_len + 1, d_model))\n",
    "\n",
    "        # Stack of NeuroBiMamba blocks\n",
    "        self.mamba_blocks = self._build_mamba_stack(n_layer, d_model, d_state, expand)\n",
    "\n",
    "        # Final layer norm\n",
    "        self.norm_f = nn.LayerNorm(d_model)\n",
    "\n",
    "    def _build_mamba_stack(self, n_layer: int, d_model: int, d_state: int, expand: int) -> nn.Sequential:\n",
    "        \"\"\"Create a sequential stack of NeuroBiMambaBlock modules.\"\"\"\n",
    "        blocks = [\n",
    "            NeuroBiMambaBlock(d_model=d_model, d_state=d_state, expand=expand)\n",
    "            for _ in range(n_layer)\n",
    "        ]\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def _prepend_cls_token(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Prepend the CLS token to a batch of patch embeddings.\"\"\"\n",
    "        B = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, d_model)\n",
    "        return torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, C, T)\n",
    "        returns: (B, d_model)  -- embedding for CLS token\n",
    "        \"\"\"\n",
    "        # Embed patches\n",
    "        x = self.patch_embed(x)  # (B, NumPatches, d_model)\n",
    "\n",
    "        # Prepend CLS token\n",
    "        x = self._prepend_cls_token(x)  # (B, 1 + NumPatches, d_model)\n",
    "\n",
    "        # Optional positional embeddings (truncate to current length)\n",
    "        if self.use_pos_embed:\n",
    "            seq_len = x.shape[1]\n",
    "            x = x + self.pos_embed[:, :seq_len]\n",
    "\n",
    "        # Pass through Mamba blocks and final norm\n",
    "        x = self.mamba_blocks(x)\n",
    "        x = self.norm_f(x)\n",
    "\n",
    "        # Return CLS representation\n",
    "        return x[:, 0]\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b50a0086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = torch.randn(8, 129, 1000).to(device)  # Example input: (B, C, T)\n",
    "model = EegMambaJEPA(\n",
    "    d_model = 256,\n",
    "    n_layer = 8,\n",
    "    n_channels = 129,\n",
    "    patch_size = 10,\n",
    "    d_state = 256,\n",
    "    expand = 4,\n",
    "    use_pos_embed= False,\n",
    "    max_len = 1000,\n",
    "\n",
    ").to(device)\n",
    "out = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db1d97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VICReg(nn.Module):\n",
    "    def __init__(self, d_model = D_MODEL_DEFAULT, \n",
    "                       lambda_val = VICREG_LAMBDA_DEFAULT, \n",
    "                       mu_val = VICREG_MU_DEFAULT,\n",
    "                       nu_val = VICREG_NU_DEFAULT,  \n",
    "                       eps=1e-4):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.lambda_val = lambda_val\n",
    "        self.mu_val = mu_val\n",
    "        self.nu_val = nu_val\n",
    "        self.eps = eps\n",
    "\n",
    "        # Projector used during pre-training\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, z1, z2):\n",
    "        z1p = self.projector(z1)\n",
    "        z2p = self.projector(z2)\n",
    "\n",
    "        # Invariance term (MSE)\n",
    "        repr_loss = nn.functional.mse_loss(z1p, z2p)\n",
    "\n",
    "        # Variance term (Hinge Loss on std dev)\n",
    "        z1_norm = z1p - z1p.mean(dim = 0)\n",
    "        z2_norm = z2p - z2p.mean(dim = 0)\n",
    "        std_z1 = torch.sqrt(z1_norm.var(dim=0) + self.eps) \n",
    "        std_z2 = torch.sqrt(z2_norm.var(dim=0) + self.eps)\n",
    "        std_loss = (torch.mean(nn.functional.relu(1 - std_z1)) + \\\n",
    "                     torch.mean(nn.functional.relu(1 - std_z2))) / 2\n",
    "\n",
    "        # Covariance term (L2 norm of off-diagonal elements)\n",
    "        B, D = z1_norm.shape\n",
    "        cov_z1 = (z1_norm.T @ z1_norm) / (B - 1)\n",
    "        cov_z2 = (z2_norm.T @ z2_norm) / (B - 1)\n",
    "\n",
    "        # Create a mask for off-diagonal elements\n",
    "        off_diag_mask = ~torch.eye(D, device=z1_norm.device).bool()\n",
    "\n",
    "        # Sum squared off-diagonal elements and normalize by dimension\n",
    "        cov_loss = (cov_z1[off_diag_mask].pow(2).sum() / D +\n",
    "                    cov_z2[off_diag_mask].pow(2).sum() / D) / 2\n",
    "\n",
    "        # Combine losses\n",
    "        loss = (self.lambda_val * repr_loss + \n",
    "                self.mu_val * std_loss +\n",
    "                self.nu_val * cov_loss)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b40d66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vicreg = VICReg(d_model = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488420d1",
   "metadata": {},
   "source": [
    "Testing why Pushing Negative Away reduces the representation collapse in self-supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70d4d08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sim: 0.987\n",
      "Negative sim: -0.272, -0.224\n",
      "Loss: 0.157\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Real features: similar within pair, different across\n",
    "z1a = torch.tensor([0.9, 0.1, -0.2]).unsqueeze(0)  # Dog-like\n",
    "z1b = torch.tensor([0.8, 0.2, -0.1]).unsqueeze(0)  # Augmented dog\n",
    "z2a = torch.tensor([-0.3, 0.9, 0.4]).unsqueeze(0)  # Cat-like\n",
    "z2b = torch.tensor([-0.2, 0.8, 0.5]).unsqueeze(0)  # Augmented cat\n",
    "\n",
    "# # Normalize for cosine similarity\n",
    "z1a = F.normalize(z1a)\n",
    "z1b = F.normalize(z1b)\n",
    "z2a = F.normalize(z2a)\n",
    "z2b = F.normalize(z2b)\n",
    "\n",
    "# Similarities\n",
    "sim_pos = F.cosine_similarity(z1a, z1b)  # ~0.95 (high)\n",
    "sim_neg1 = F.cosine_similarity(z1a, z2a) # ~-0.2 (low)\n",
    "sim_neg2 = F.cosine_similarity(z1a, z2b) # ~-0.1 (low)\n",
    "\n",
    "print(f\"Positive sim: {sim_pos.item():.3f}\")\n",
    "print(f\"Negative sim: {sim_neg1.item():.3f}, {sim_neg2.item():.3f}\")\n",
    "\n",
    "# Loss calculation (tau=0.5)\n",
    "num = torch.exp(sim_pos / 0.5)     # exp(0.95/0.5) = exp(1.9) ≈ 6.69\n",
    "den = num + torch.exp(sim_neg1/0.5) + torch.exp(sim_neg2/0.5)  # ~6.69 + 0.74 + 0.82 ≈ 8.25\n",
    "loss = -torch.log(num / den)       # -log(6.69/8.25) ≈ 0.21\n",
    "\n",
    "print(f\"Loss: {loss.item():.3f}\")  # Much lower than collapsed case!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "167bc189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MDN Head & Loss (Loss function included for reference) ---\n",
    "class MDNHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture Density Network head that predicts parameters of a 1-D Gaussian\n",
    "    mixture for each input vector.\n",
    "\n",
    "    Inputs:\n",
    "      x: Tensor of shape (B, input_dim)\n",
    "\n",
    "    Outputs (each tensor shape (B, n_components)):\n",
    "      pi    : mixture weights (probabilities that sum to 1 across components)\n",
    "      sigma : positive standard deviations (softplus output + eps)\n",
    "      mu    : component means\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_dim: int = D_MODEL_DEFAULT,  \n",
    "                 n_components: int = MDN_COMPONENTS_DEFAULT,\n",
    "                 min_sigma: float = 1e-6):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.n_components = n_components\n",
    "        self.min_sigma = min_sigma\n",
    "\n",
    "\n",
    "        # Separate linear heads for mixture logits, means and (unconstrained) scale\n",
    "        self.pi = nn.Linear(input_dim, n_components)      \n",
    "        self.sigma = nn.Linear(input_dim, n_components)  \n",
    "        self.mu = nn.Linear(input_dim, n_components)       \n",
    "\n",
    "        # Softplus is a smooth positive function for sigma stability\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Predict MDN parameters \n",
    "\n",
    "        Returns:\n",
    "            pi    : mixture weights (B, n_components)\n",
    "            sigma : standard deviations (B, n_components)\n",
    "            mu    : component means (B, n_components)\n",
    "        \"\"\"\n",
    "\n",
    "        # Mixture logits -> probabilities\n",
    "        pi_logits = self.pi(x)\n",
    "        pi = torch.softmax(pi_logits, dim=1)\n",
    "\n",
    "        # Means\n",
    "        mu  = self.mu(x)\n",
    "\n",
    "        # Positive scales via softflus for numerical stability\n",
    "        sigma = self.softplus(self.sigma(x)) + self.min_sigma\n",
    "\n",
    "        return pi, sigma, mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c434d56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
